{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ting1029/HelloWorld/blob/main/%E3%80%8CGroq_01_quickstart_ipynb%E3%80%8D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904zvdxBPQuB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "904zvdxBPQuB",
        "outputId": "4a544f0d-97f9-4d89-92bd-b2c409699e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/LLM_Quickstart\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# change to your working directory\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/LLM_Quickstart #目錄(LLM_Quickstart)要改\n",
        "\n",
        "# Creat a new environment\n",
        "# !virtualenv colab_env\n",
        "\n",
        "#!chmod 744 /content/drive/MyDrive/Colab\\ Notebooks/colab_env/bin/pip\n",
        "# activate colab_env\n",
        "!source /content/drive/MyDrive/Colab\\ Notebooks/colab_env/bin/activate\n",
        "\n",
        "# add new env site-packages to PATH\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/colab_env/lib/python3.10/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h93-oCHyO-b1",
      "metadata": {
        "id": "h93-oCHyO-b1"
      },
      "outputs": [],
      "source": [
        "# Code to install libraries\n",
        "#!chmod 744 /content/drive/MyDrive/Colab\\ Notebooks/colab_env/bin/pip\n",
        "# activate colab_env\n",
        "# !source /content/drive/MyDrive/Colab\\ Notebooks/colab_env/bin/activate; pip install groq openai markdown2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W6ERAgztO-b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "W6ERAgztO-b6",
        "outputId": "8db88f87-76a2-4cf1-fe5f-48d75186abe6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-816927298692>, line 32)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-816927298692>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    print(response\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "\n",
        "# Set API keys as custom variables\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set API Keys as Environment Variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"]  = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "# Create a client\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Make a chat conversation\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-8b-instant\", #groq 模型\n",
        "    #model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "response = chat_completion.choices[0].message.content\n",
        "print(response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v6E2gq6Ma2e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6E2gq6Ma2e9",
        "outputId": "f3cf3974-ccfe-4d5a-8337-9c559633cc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='Fast language models have gained significant attention in recent years due to their potential to revolutionize various applications and industries. Here are some key reasons why fast language models are important:\\n\\n1. **Scalability and Efficient Inference**: Traditional language models can be computationally expensive, making it challenging to deploy them on-demand. Fast language models are designed to be efficient and scalable, allowing them to handle a large volume of requests, making them suitable for real-time applications.\\n2. **Real-time Conversational AI**: Fast language models enable the development of real-time conversational AI systems, such as chatbots, voice assistants, and virtual customer agents. These systems can understand and respond to user queries at the speed of conversation, providing a seamless user experience.\\n3. **Low-Latency Edge Computing**: Fast language models can be deployed on edge devices, such as smartphones, smart speakers, or autonomous vehicles, where low-latency is critical. By processing language understanding and generation locally, these models can reduce latency and improve responsiveness.\\n4. **Edge-AI and IoT Applications**: Fast language models are integrated into edge-AI and IoT applications, enabling devices to understand and respond to voice commands, text inputs, or sensor data in real-time. This enhances the overall user experience and facilitates new use cases.\\n5. **Improved User Experience**: Fast language models can provide faster response times, more accurate responses, and a more intuitive user experience. This is particularly important in applications like customer service, language translation, and content creation, where speed and accuracy are crucial.\\n6. **Domain-Specific Applications**: Fast language models can be fine-tuned for domain-specific tasks, such as medical diagnosis, financial analysis, or product recommendations. These models can process complex queries and provide relevant responses quickly, enhancing decision-making processes.\\n7. **Increased Throughput**: Fast language models can handle a high volume of requests, enabling organizations to handle large-scale conversational workflows, customer inquiries, or content generation tasks.\\n8. **Reduced Server Costs**: By deploying fast language models on edge devices or in cloud providers, organizations can reduce their server costs and minimize the need for additional infrastructure.\\n9. **Competitive Advantage**: Companies that adopt fast language models can gain a competitive advantage in the market, enabling them to provide faster, more accurate, and more personalized services to their customers.\\n10. **Research and Innovation**: The pursuit of fast language models drives research and innovation in AI, NLP, and cognitive computing. By pushing the boundaries of language understanding and generation, researchers and developers can create new applications, features, and use cases that transform industries and improve lives.\\n\\nIn summary, fast language models are crucial for a wide range of applications and industries, offering significant benefits in scalability, real-time conversational AI, low-latency edge computing, and user experience. Their impact extends to edge-AI, IoT, domain-specific tasks, throughput, cost savings, and competitive advantage, ultimately driving innovation and improvement in various fields.', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "print(chat_completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMt7pM21cf88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMt7pM21cf88",
        "outputId": "0331b9ce-2da3-4100-dfc9-3f5e08fbf69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markdown2\n",
            "  Downloading markdown2-2.5.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading markdown2-2.5.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: markdown2\n",
            "Successfully installed markdown2-2.5.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install markdown2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AG-f2AhRaaru",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "AG-f2AhRaaru",
        "outputId": "eae3fc9d-db2a-48fb-84e9-82b27d7d5d11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Fast language models have gained significant attention in recent years due to their potential to revolutionize various applications and industries. Here are some key reasons why fast language models are important:</p>\n",
              "\n",
              "<ol>\n",
              "<li><strong>Scalability and Efficient Inference</strong>: Traditional language models can be computationally expensive, making it challenging to deploy them on-demand. Fast language models are designed to be efficient and scalable, allowing them to handle a large volume of requests, making them suitable for real-time applications.</li>\n",
              "<li><strong>Real-time Conversational AI</strong>: Fast language models enable the development of real-time conversational AI systems, such as chatbots, voice assistants, and virtual customer agents. These systems can understand and respond to user queries at the speed of conversation, providing a seamless user experience.</li>\n",
              "<li><strong>Low-Latency Edge Computing</strong>: Fast language models can be deployed on edge devices, such as smartphones, smart speakers, or autonomous vehicles, where low-latency is critical. By processing language understanding and generation locally, these models can reduce latency and improve responsiveness.</li>\n",
              "<li><strong>Edge-AI and IoT Applications</strong>: Fast language models are integrated into edge-AI and IoT applications, enabling devices to understand and respond to voice commands, text inputs, or sensor data in real-time. This enhances the overall user experience and facilitates new use cases.</li>\n",
              "<li><strong>Improved User Experience</strong>: Fast language models can provide faster response times, more accurate responses, and a more intuitive user experience. This is particularly important in applications like customer service, language translation, and content creation, where speed and accuracy are crucial.</li>\n",
              "<li><strong>Domain-Specific Applications</strong>: Fast language models can be fine-tuned for domain-specific tasks, such as medical diagnosis, financial analysis, or product recommendations. These models can process complex queries and provide relevant responses quickly, enhancing decision-making processes.</li>\n",
              "<li><strong>Increased Throughput</strong>: Fast language models can handle a high volume of requests, enabling organizations to handle large-scale conversational workflows, customer inquiries, or content generation tasks.</li>\n",
              "<li><strong>Reduced Server Costs</strong>: By deploying fast language models on edge devices or in cloud providers, organizations can reduce their server costs and minimize the need for additional infrastructure.</li>\n",
              "<li><strong>Competitive Advantage</strong>: Companies that adopt fast language models can gain a competitive advantage in the market, enabling them to provide faster, more accurate, and more personalized services to their customers.</li>\n",
              "<li><strong>Research and Innovation</strong>: The pursuit of fast language models drives research and innovation in AI, NLP, and cognitive computing. By pushing the boundaries of language understanding and generation, researchers and developers can create new applications, features, and use cases that transform industries and improve lives.</li>\n",
              "</ol>\n",
              "\n",
              "<p>In summary, fast language models are crucial for a wide range of applications and industries, offering significant benefits in scalability, real-time conversational AI, low-latency edge computing, and user experience. Their impact extends to edge-AI, IoT, domain-specific tasks, throughput, cost savings, and competitive advantage, ultimately driving innovation and improvement in various fields.</p>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import markdown2\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Example usage:\n",
        "output = chat_completion.choices[0].message.content  # Your provided output\n",
        "markdown_result = markdown2.markdown(output)\n",
        "display(HTML(markdown_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7l8RrUpaeeMi",
      "metadata": {
        "id": "7l8RrUpaeeMi"
      },
      "source": [
        "How to have multiple chats with the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Mrc2XDbeMx6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "3Mrc2XDbeMx6",
        "outputId": "e4453e7b-8385-4406-8f8b-1f508f2ebe7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Here are five key points highlighting the importance of fast language models:</p>\n",
              "\n",
              "<ol>\n",
              "<li><p><strong>Efficient Processing Power</strong>: Fast language models optimize the use of processing power, which enables them to support real-time applications such as language translation, chatbots, and virtual assistants. This translates to improved user experience and usability.</p></li>\n",
              "<li><p><strong>Streamlined Computational Tasks</strong>: The efficiency in these models implies it being able to crunch large data speeds in real-time. This capacity is especially helpful in business cases where meeting quick deadlines (e.g., real-time analytics) is important.</p></li>\n",
              "<li><p><strong>Increased Scalability</strong>: Capacity to support real-time tasks at low-level processing power enables these models to scale more effectively, therefore, facilitating broader integration in any domain requiring high data processing, such as IT solutions.</p></li>\n",
              "<li><p><strong>Quality and Accurateness</strong>: Although these are naturally high-efficient and time-saving, the primary importance might be due to their quality output being as good or as comparable to more expensive alternatives (therefore useful for time-sensitive commercial projects). In practical terms when looking at generating an entire article just using provided information, this matters even more.</p></li>\n",
              "<li><p><strong>Competitive Edge</strong>: Lastly, one major point is how fast language models give organizations a competitive advantage in terms of automation capabilities and use of these tools broadly across companies, including small business (micro scale operations). By leveraging similar tools at cost-effective premiums, businesses can get to innovate aggressively.</p></li>\n",
              "</ol>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define a chart with llm function\n",
        "def chat_with_llm(messages):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Example usage for multiple chats:\n",
        "messages1 = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain the importance of fast language models in five items.\"},\n",
        "]\n",
        "\n",
        "def display_llm_response(response):\n",
        "    markdown_result = markdown2.markdown(response)\n",
        "    display(HTML(markdown_result))\n",
        "\n",
        "response1 = chat_with_llm(messages1)\n",
        "display_llm_response(response1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-wrh_KQ7ez5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "-wrh_KQ7ez5f",
        "outputId": "59fdf956-1ca5-4012-f192-881c8b8e10ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chat_with_llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1a11c1f35de0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What are the limitations of current LLMs in three key points with shorter answer?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ]\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdisplay_llm_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chat_with_llm' is not defined"
          ]
        }
      ],
      "source": [
        "messages2 = [\n",
        "    {\"role\": \"user\", \"content\": \"What are the limitations of current LLMs in three key points with shorter answer?\"},\n",
        "]\n",
        "response2 = chat_with_llm(messages2)\n",
        "display_llm_response(response2)\n",
        "\n",
        "# ... (Add more chats as needed) ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}